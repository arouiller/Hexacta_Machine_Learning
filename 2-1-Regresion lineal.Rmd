---
title: "Regresion lineal"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Regresión lineal

### Limpiamos memoria
```{r}
rm(list=ls())
```

### Generación de datos aleatorios
Generamos un conjunto de datos aleatorios
```{r}
x <- runif(1000, -5, 5)
y <- x + rnorm(1000) + 3
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Regresión lineal con descenso de gradiente')
```

### Modelo lineal

El modelo lineal optimo es calculado en forma matemática.
El valor de m y b son a los que debería llegar, en los siguientes pasos, el algoritmo de aprendizaje

```{r}
res <- lm( y ~ x )

m <- round(as.numeric(res$coefficients['(Intercept)']), 3)
b <- round(as.numeric(res$coefficients['x']), 3)

c(m, b)

titulo = paste('Regresion lineal (y=',m,'+',b," * x)", sep='')

plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main= titulo) + abline(res, col='blue')
```



### Implementacion

#### Funcion de error
Esta es la funcion de error que el algorimo debe minimizar.
El error está calculado como la sumatoria al cuadrado de las diferencias entre el valor predicho y el valor real, dividido por 2 veces la cantidad de datos
```{r}
cost <- function(X, y, theta) {
  sum( (X %*% theta - y)^2 ) / (2*length(y))
}
```

#### Hiper parámetros
Nuestra implementación utiliza dos hiper parámetros
alpha: la tasa de crecimiento
num_iters: la cantidad de iteraciones que se realizarán

Por simplicidad se ha eliminado el parámetro epsilon (que mide la variación que hubo entre el valor predicho en la iteración n-1 y el valor predicho en la iteracion n)

```{r}
  # Tasa de crecimiento
  alpha <- 0.01
  
  # Número máximo de iteraciones
  num_iters <- 250
```

#### Implementacion
Generamos dos listas para mantener los valores históricos calculados de costo y de los parámetros hallados
```{r}
cost_history <- double(num_iters)
theta_history <- list(num_iters)
```

Generamos los primeros parametros en forma aleatoria
```{r}
theta <- matrix(c(runif(1, -5, 5), runif(1, -5, 5)), nrow=2)

# agregamos una columna de valor 1
X <- cbind(1, matrix(x))
i <-1
# Descenso del gradiente
while(i <= num_iters ){
  #Error en lo predicho vs lo real
  error <- (X %*% theta - y)
  
  delta <- t(X) %*% error / length(y)
  #Nuevos parametros
  theta <- theta - alpha * delta
  
  #Almacenamos el costo
  cost_history[i] <- cost(X, y, theta)
  #Almacenamos los parametros
  theta_history[[i]] <- theta
  i <- i+1
}

m <- round(theta[1,1], 3)
b <- round(theta[2,1], 3)

titulo = paste('Linear regression by gradient descent (y= ',m,' + ',b,"*x)", sep='')

plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main=titulo)
for (i in 1:num_iters) {
  color = ifelse(i==1, 'green', 'red')
  abline(coef=theta_history[[i]], col=color)
}
abline(coef=theta, col='blue')
```

